{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "788934e4",
   "metadata": {},
   "source": [
    "# SEMMA Methodology applied to Wine Quality Dataset\n",
    "In this notebook, we follow the SEMMA methodology to analyze and build a machine learning model for the Wine Quality dataset. The SEMMA process includes:\n",
    "- **Sample**: Using the full dataset since it's manageable in size.\n",
    "- **Explore**: Analyzing the data to find trends, correlations, and distribution of features.\n",
    "- **Modify**: Cleaning the data and preparing it for modeling, including feature scaling.\n",
    "- **Model**: Training a Random Forest Classifier to predict wine quality.\n",
    "- **Assess**: Evaluating the model's performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02af7d51",
   "metadata": {},
   "source": [
    "## Step 1: Sample\n",
    "We will use the full dataset as it contains 1143 rows, which is manageable for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39218be6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing required libraries\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "# Load the dataset\n",
    "file_path = '/content/WineQT.csv'\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Display first few rows of the dataset\n",
    "data.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34ba609f",
   "metadata": {},
   "source": [
    "## Step 2: Explore\n",
    "We will perform Exploratory Data Analysis (EDA) to better understand the dataset and its relationships."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07496d88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Descriptive statistics and correlation heatmap\n",
    "desc_stats = data.describe()\n",
    "print(desc_stats)\n",
    "\n",
    "# Correlation matrix\n",
    "correlation_matrix = data.corr()\n",
    "\n",
    "# Plot correlation heatmap\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', linewidths=0.5)\n",
    "plt.title('Correlation Heatmap of Wine Quality Dataset')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31f8c6e1",
   "metadata": {},
   "source": [
    "## Step 3: Modify\n",
    "In this step, we will drop the 'Id' column (as it is not relevant), scale the features, and split the data into training and testing sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77d11d6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping the 'Id' column\n",
    "data_cleaned = data.drop('Id', axis=1)\n",
    "\n",
    "# Separate the features and target variable\n",
    "X = data_cleaned.drop('quality', axis=1)\n",
    "y = data_cleaned['quality']\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Feature scaling\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d39c57c",
   "metadata": {},
   "source": [
    "## Step 4: Model\n",
    "We will train a Random Forest classifier on the training data to predict the wine quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1815d9b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Random Forest Classifier\n",
    "model = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predict on the test data\n",
    "y_pred = model.predict(X_test_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66dd2498",
   "metadata": {},
   "source": [
    "## Step 5: Assess\n",
    "Now, we evaluate the model's performance using accuracy score, confusion matrix, and classification report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1df822e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "class_report = classification_report(y_test, y_pred)\n",
    "\n",
    "# Output the evaluation results\n",
    "print(f'Accuracy: {accuracy}')\n",
    "print(f'Confusion Matrix:\\n{conf_matrix}')\n",
    "print(f'Classification Report:\\n{class_report}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a58eeb38",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "The model achieved around 69.87% accuracy, but there is room for improvement. Techniques such as handling class imbalance and hyperparameter tuning could further enhance the model's performance."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
