{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6933559b",
   "metadata": {},
   "source": [
    "# Heart Attack Prediction using Logistic Regression and Random Forest\n",
    "\n",
    "Following CRISP-DM methodology to predict heart attacks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c9e6d4f",
   "metadata": {},
   "source": [
    "## Step 1: Importing necessary libraries\n",
    "\n",
    "We first import the required libraries for data manipulation, model training, and evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b46d0f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "import joblib\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff371265",
   "metadata": {},
   "source": [
    "## Step 2: Loading the Dataset\n",
    "\n",
    "Load the heart attack dataset from a CSV file and display the first few rows to understand the structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af6e8c41",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load dataset\n",
    "file_path = '/content/heart.csv'\n",
    "heart_data = pd.read_csv(file_path)\n",
    "\n",
    "# Step 2: Data Understanding\n",
    "# Display first few rows of the dataset\n",
    "heart_data.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e008a1bc",
   "metadata": {},
   "source": [
    "## Step 3: Data Preparation\n",
    "\n",
    "In this step, we define our features and target variable. We also split the data into training and testing sets and apply scaling to ensure that the features are properly normalized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2675a61",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Step 3: Data Preparation\n",
    "# Define features and target variable\n",
    "X = heart_data.drop(columns='output')\n",
    "y = heart_data['output']\n",
    "\n",
    "# Split data into training and testing sets (80% train, 20% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Apply StandardScaler to continuous variables for scaling\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f16c219",
   "metadata": {},
   "source": [
    "## Step 4: Model Training\n",
    "\n",
    "We will train two machine learning models: Logistic Regression and Random Forest. Both models will be trained using the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a269b9b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Step 4: Modeling - Logistic Regression and Random Forest\n",
    "log_reg = LogisticRegression()\n",
    "rf_clf = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# Train models on the training data\n",
    "log_reg.fit(X_train_scaled, y_train)\n",
    "rf_clf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predict on the test data\n",
    "log_reg_preds = log_reg.predict(X_test_scaled)\n",
    "rf_clf_preds = rf_clf.predict(X_test_scaled)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "447b2653",
   "metadata": {},
   "source": [
    "## Step 5: Model Evaluation\n",
    "\n",
    "We evaluate the models using metrics such as accuracy, precision, recall, F1-score, and ROC-AUC. This helps us understand how well the models are performing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68371082",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Step 5: Evaluation\n",
    "def evaluate_model(y_test, predictions):\n",
    "    return {\n",
    "        \"Accuracy\": accuracy_score(y_test, predictions),\n",
    "        \"Precision\": precision_score(y_test, predictions),\n",
    "        \"Recall\": recall_score(y_test, predictions),\n",
    "        \"F1-Score\": f1_score(y_test, predictions),\n",
    "        \"ROC-AUC\": roc_auc_score(y_test, predictions)\n",
    "    }\n",
    "\n",
    "# Get evaluation metrics for Logistic Regression and Random Forest\n",
    "log_reg_metrics = evaluate_model(y_test, log_reg_preds)\n",
    "rf_clf_metrics = evaluate_model(y_test, rf_clf_preds)\n",
    "\n",
    "# Display the evaluation results\n",
    "print(\"Logistic Regression Metrics:\", log_reg_metrics)\n",
    "print(\"Random Forest Metrics:\", rf_clf_metrics)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "286899c6",
   "metadata": {},
   "source": [
    "## Step 6: Model Deployment\n",
    "\n",
    "We save the Logistic Regression model as a file so it can be deployed in a real-world system for making predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce841193",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Step 6: Deployment\n",
    "# Exporting the Logistic Regression model\n",
    "model_file_path = '/content/logistic_regression_heart_attack_model.pkl'\n",
    "joblib.dump(log_reg, model_file_path)\n",
    "\n",
    "print(\"Model saved as logistic_regression_heart_attack_model.pkl\")\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
